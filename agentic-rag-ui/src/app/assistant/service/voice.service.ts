// ============================================================================
// SERVICE - voice.service.ts (ADAPT√â POUR WHISPER)
// ============================================================================
import { Injectable } from '@angular/core';
import { HttpClient } from '@angular/common/http';
import { Observable, Subject } from 'rxjs';
import { environment } from '../../../../environements/environement';

/**
 * ‚úÖ Service vocal avec OpenAI Whisper
 * Enregistrement audio ‚Üí Backend ‚Üí Whisper API ‚Üí Transcription
 */
@Injectable({
  providedIn: 'root'
})
export class VoiceService {
  private readonly API_URL = 'http://localhost:8090/api';
  
  // ==================== ENREGISTREMENT AUDIO ====================
  
  private mediaRecorder: MediaRecorder | null = null;
  private audioChunks: Blob[] = [];
  private recordingSubject = new Subject<boolean>();
  private errorSubject = new Subject<string>();
  
  constructor(private http: HttpClient) {}
  
  /**
   * ‚úÖ V√©rifie si l'enregistrement audio est support√©
   */
  isRecordingSupported(): boolean {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
  }
  
  /**
   * ‚úÖ D√©marre l'enregistrement audio
   */
  async startRecording(): Promise<void> {
    try {
      console.log('üé§ [VoiceService] Demande acc√®s microphone');
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 16000
        } 
      });
      
      console.log('‚úÖ [VoiceService] Acc√®s microphone accord√©');
      
      // Cr√©er le MediaRecorder
      const options = { mimeType: 'audio/webm' };
      this.mediaRecorder = new MediaRecorder(stream, options);
      this.audioChunks = [];
      
      // Collecter les donn√©es audio
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.audioChunks.push(event.data);
          console.log('üìä [VoiceService] Chunk audio re√ßu:', event.data.size, 'bytes');
        }
      };
      
      // D√©marrer l'enregistrement
      this.mediaRecorder.start(1000); // Collecte toutes les secondes
      this.recordingSubject.next(true);
      
      console.log('üé§ [VoiceService] Enregistrement d√©marr√©');
      
    } catch (error: any) {
      console.error('‚ùå [VoiceService] Erreur acc√®s microphone:', error);
      this.errorSubject.next(error.message || 'Erreur acc√®s microphone');
      throw error;
    }
  }
  
  /**
   * ‚úÖ Arr√™te l'enregistrement et retourne le blob audio
   */
  async stopRecording(): Promise<Blob> {
    return new Promise((resolve, reject) => {
      if (!this.mediaRecorder) {
        reject(new Error('Aucun enregistrement en cours'));
        return;
      }
      
      console.log('üõë [VoiceService] Arr√™t enregistrement');
      
      this.mediaRecorder.onstop = () => {
        // Cr√©er le blob audio
        const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
        console.log('‚úÖ [VoiceService] Enregistrement arr√™t√©:', audioBlob.size, 'bytes');
        
        // Arr√™ter tous les tracks du microphone
        if (this.mediaRecorder?.stream) {
          this.mediaRecorder.stream.getTracks().forEach(track => {
            track.stop();
            console.log('üîá [VoiceService] Track audio arr√™t√©');
          });
        }
        
        this.recordingSubject.next(false);
        resolve(audioBlob);
      };
      
      this.mediaRecorder.onerror = (error: any) => {
        console.error('‚ùå [VoiceService] Erreur MediaRecorder:', error);
        this.errorSubject.next('Erreur lors de l\'enregistrement');
        reject(error);
      };
      
      this.mediaRecorder.stop();
    });
  }
  
  /**
   * ‚úÖ Transcrit l'audio avec Whisper via le backend
   * 
   * @param audioBlob Blob audio √† transcrire
   * @param language Code langue (fr, en, es, etc.)
   * @returns Observable de la r√©ponse API
   */
  transcribeWithWhisper(audioBlob: Blob, language: string = 'fr'): Observable<WhisperResponse> {
    const formData = new FormData();
    formData.append('audio', audioBlob, 'recording.webm');
    formData.append('language', language);
    
    console.log('üì§ [VoiceService] Envoi audio √† Whisper');
    console.log('üìä [VoiceService] Taille:', audioBlob.size, 'bytes');
    console.log('üåç [VoiceService] Langue:', language);
    
    return this.http.post<WhisperResponse>(
      this.API_URL + `/voice/transcribe`,
      formData
    );
  }
  
  /**
   * ‚úÖ Observable de l'√©tat d'enregistrement
   */
  getRecordingState(): Observable<boolean> {
    return this.recordingSubject.asObservable();
  }
  
  /**
   * ‚úÖ Observable des erreurs
   */
  getErrors(): Observable<string> {
    return this.errorSubject.asObservable();
  }
  
  /**
   * ‚úÖ V√©rifie si en cours d'enregistrement
   */
  isRecording(): boolean {
    return this.mediaRecorder?.state === 'recording';
  }
}

/**
 * ‚úÖ Interface de r√©ponse Whisper
 */
export interface WhisperResponse {
  success: boolean;
  transcript: string;
  language: string;
  audioSize: number;
  filename: string;
  transcriptLength?: number;
  error?: string;
}